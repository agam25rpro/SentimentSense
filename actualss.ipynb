{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1808590,"sourceType":"datasetVersion","datasetId":989445}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:32:05.523847Z","iopub.execute_input":"2025-01-18T08:32:05.524121Z","iopub.status.idle":"2025-01-18T08:32:05.537649Z","shell.execute_reply.started":"2025-01-18T08:32:05.524098Z","shell.execute_reply":"2025-01-18T08:32:05.536642Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/sentiment-analysis-dataset/training.1600000.processed.noemoticon.csv\n/kaggle/input/sentiment-analysis-dataset/train.csv\n/kaggle/input/sentiment-analysis-dataset/testdata.manual.2009.06.14.csv\n/kaggle/input/sentiment-analysis-dataset/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# [1] Import Libraries\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:32:05.538511Z","iopub.execute_input":"2025-01-18T08:32:05.538848Z","iopub.status.idle":"2025-01-18T08:32:05.548367Z","shell.execute_reply.started":"2025-01-18T08:32:05.538818Z","shell.execute_reply":"2025-01-18T08:32:05.547764Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport nltk\n\n# Define the custom directory path\ncustom_nltk_data_path = '/kaggle/working/nltk_data'\n\n# Create the directory if it doesn't exist\nos.makedirs(custom_nltk_data_path, exist_ok=True)\n\n# Set the NLTK data path to the custom directory\nnltk.data.path.append(custom_nltk_data_path)\n\n# Download the necessary NLTK resources\nnltk.download('punkt', download_dir=custom_nltk_data_path)\nnltk.download('stopwords', download_dir=custom_nltk_data_path)\nnltk.download('wordnet', download_dir=custom_nltk_data_path)\nnltk.download('omw-1.4', download_dir=custom_nltk_data_path)\n\n# Verify if the resources are downloaded\nprint(\"NLTK resources downloaded to:\", custom_nltk_data_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:32:05.549964Z","iopub.execute_input":"2025-01-18T08:32:05.550267Z","iopub.status.idle":"2025-01-18T08:32:05.731621Z","shell.execute_reply.started":"2025-01-18T08:32:05.550241Z","shell.execute_reply":"2025-01-18T08:32:05.730762Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /kaggle/working/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\nNLTK resources downloaded to: /kaggle/working/nltk_data\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport nltk\n\n# Define the custom directory path\ncustom_nltk_data_path = '/kaggle/working/nltk_data'\n\n# Create the directory if it doesn't exist\nos.makedirs(custom_nltk_data_path, exist_ok=True)\n\n# Set the NLTK data path to the custom directory\nnltk.data.path.append(custom_nltk_data_path)\n\n# Manually download the required resource\nnltk.download('wordnet', download_dir=custom_nltk_data_path)\n\n# Verify if NLTK can find the resource\ntry:\n    wordnet = nltk.corpus.wordnet\n    print(\"WordNet resource found.\")\nexcept LookupError as e:\n    print(f\"Resource not found: {e}\")\n\n# Check the paths being searched by NLTK\nprint(\"Paths searched by NLTK:\")\nprint(nltk.data.path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:32:05.732918Z","iopub.execute_input":"2025-01-18T08:32:05.733204Z","iopub.status.idle":"2025-01-18T08:32:05.762258Z","shell.execute_reply.started":"2025-01-18T08:32:05.733184Z","shell.execute_reply":"2025-01-18T08:32:05.761620Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nWordNet resource found.\nPaths searched by NLTK:\n['/root/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/kaggle/working/nltk_data', '/kaggle/working/nltk_data']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import zipfile\nimport os\n\n# Define the path where the zipped files are located\nnltk_data_path = '/kaggle/working/nltk_data'\n\n# List of zipped files in your nltk_data path\nzip_files = ['stopwords.zip', 'wordnet.zip', 'omw-1.4.zip']\n\n# Unzip each file\nfor zip_file in zip_files:\n    zip_path = os.path.join(nltk_data_path, 'corpora', zip_file)\n    if os.path.exists(zip_path):\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(os.path.join(nltk_data_path, 'corpora'))\n        print(f\"Unzipped {zip_file}\")\n    else:\n        print(f\"{zip_file} not found.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:32:05.763138Z","iopub.execute_input":"2025-01-18T08:32:05.763429Z","iopub.status.idle":"2025-01-18T08:32:06.658384Z","shell.execute_reply.started":"2025-01-18T08:32:05.763401Z","shell.execute_reply":"2025-01-18T08:32:06.657435Z"}},"outputs":[{"name":"stdout","text":"Unzipped stopwords.zip\nUnzipped wordnet.zip\nUnzipped omw-1.4.zip\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# [3] Initialize preprocessing tools\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:32:06.659496Z","iopub.execute_input":"2025-01-18T08:32:06.659830Z","iopub.status.idle":"2025-01-18T08:32:06.667669Z","shell.execute_reply.started":"2025-01-18T08:32:06.659801Z","shell.execute_reply":"2025-01-18T08:32:06.666816Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# [4] Enhanced preprocessing function\ndef preprocess_text(text):\n    \"\"\"\n    Comprehensive text preprocessing for tweets\n    \"\"\"\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n    text = re.sub(r'<.*?>', '', text)\n    text = re.sub(r'@\\w+', '@user', text)\n    text = re.sub(r'#(\\w+)', r'\\1', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\d+', '', text)\n    tokens = word_tokenize(text)\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n    return ' '.join(tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:32:06.668483Z","iopub.execute_input":"2025-01-18T08:32:06.668769Z","iopub.status.idle":"2025-01-18T08:32:06.684629Z","shell.execute_reply.started":"2025-01-18T08:32:06.668749Z","shell.execute_reply":"2025-01-18T08:32:06.683740Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# [5] Model Architecture Classes\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n        \n        if mask is not None:\n            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n            \n        attention_weights = F.softmax(attention_scores, dim=-1)\n        output = torch.matmul(attention_weights, V)\n        return output, attention_weights\n    \n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        output = self.W_o(output)\n        return output, attention_weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:32:06.686994Z","iopub.execute_input":"2025-01-18T08:32:06.687222Z","iopub.status.idle":"2025-01-18T08:32:06.699946Z","shell.execute_reply.started":"2025-01-18T08:32:06.687197Z","shell.execute_reply":"2025-01-18T08:32:06.699096Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# [7] Load and preprocess data\ndef load_twitter_data(filepath):\n    \"\"\"Load and preprocess Twitter sentiment dataset with enhanced preprocessing\"\"\"\n    print(\"Loading and preprocessing data...\")\n    df = pd.read_csv(filepath, encoding='windows-1252')\n    df = df.dropna(subset=['text'])\n    df['text'] = df['text'].apply(preprocess_text)\n    sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n    df['sentiment'] = df['sentiment'].map(sentiment_map)\n    df = df[df['text'].str.len() > 0].reset_index(drop=True)\n    train_texts, val_texts, train_labels, val_labels = train_test_split(\n        df['text'].values,\n        df['sentiment'].values,\n        test_size=0.2,\n        random_state=42,\n        stratify=df['sentiment']\n    )\n    print(f\"Training samples: {len(train_texts)}\")\n    print(f\"Validation samples: {len(val_texts)}\")\n    return train_texts, val_texts, train_labels, val_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:32:06.701114Z","iopub.execute_input":"2025-01-18T08:32:06.701474Z","iopub.status.idle":"2025-01-18T08:32:06.718298Z","shell.execute_reply.started":"2025-01-18T08:32:06.701447Z","shell.execute_reply":"2025-01-18T08:32:06.717439Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# [8] Training with learning rate scheduling and gradient clipping\ndef train_model(model, train_loader, val_loader, epochs=10, learning_rate=0.0001, device='cuda'):\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n    criterion = nn.CrossEntropyLoss()\n    best_val_loss = float('inf')\n    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n            optimizer.zero_grad()\n            outputs = model(input_ids)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            total_loss += loss.item()\n        avg_train_loss = total_loss / len(train_loader)\n        history['train_loss'].append(avg_train_loss)\n        model.eval()\n        val_loss = 0\n        predictions, true_labels = [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids = batch['input_ids'].to(device)\n                labels = batch['labels'].to(device)\n                outputs = model(input_ids)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                preds = torch.argmax(outputs, dim=1)\n                predictions.extend(preds.cpu().numpy())\n                true_labels.extend(labels.cpu().numpy())\n        avg_val_loss = val_loss / len(val_loader)\n        history['val_loss'].append(avg_val_loss)\n        accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n        history['val_accuracy'].append(accuracy)\n        scheduler.step(avg_val_loss)\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': best_val_loss,\n            }, 'best_model.pt')\n    return history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:32:06.719129Z","iopub.execute_input":"2025-01-18T08:32:06.719428Z","iopub.status.idle":"2025-01-18T08:32:06.739183Z","shell.execute_reply.started":"2025-01-18T08:32:06.719398Z","shell.execute_reply":"2025-01-18T08:32:06.738407Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# First, we need the positional encoding class\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length=512):\n        super().__init__()\n        \n        # Create a matrix of positional encodings\n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        \n        # Apply sine to even indices\n        pe[:, 0::2] = torch.sin(position * div_term)\n        # Apply cosine to odd indices\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\n# Then the transformer block\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        \n        # Multi-head attention layer\n        self.attention = MultiHeadAttention(d_model, num_heads)\n        # Layer normalization\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        # Feed-forward neural network\n        self.feed_forward = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model)\n        )\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        # Apply attention and add residual connection\n        attention_output, _ = self.attention(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attention_output))\n        \n        # Apply feed-forward and add residual connection\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        \n        return x\n\n# Finally, the main transformer model\nclass TwitterSentimentTransformer(nn.Module):\n    def __init__(self, vocab_size, d_model=256, num_heads=8, num_layers=4, d_ff=1024, max_seq_length=128, dropout=0.1):\n        super().__init__()\n        \n        # Word embedding layer\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        # Positional encoding layer\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n        \n        # Stack of transformer blocks\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_layers)\n        ])\n        \n        # Classification layers\n        self.fc1 = nn.Linear(d_model, d_model // 2)\n        self.fc2 = nn.Linear(d_model // 2, 3)  # 3 classes: negative, neutral, positive\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        # Convert input tokens to embeddings\n        x = self.embedding(x)\n        # Add positional encoding\n        x = self.positional_encoding(x)\n        \n        # Pass through transformer blocks\n        for transformer_block in self.transformer_blocks:\n            x = transformer_block(x, mask)\n        \n        # Global average pooling\n        x = torch.mean(x, dim=1)\n        \n        # Classification\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:32:06.739990Z","iopub.execute_input":"2025-01-18T08:32:06.740327Z","iopub.status.idle":"2025-01-18T08:32:06.758439Z","shell.execute_reply.started":"2025-01-18T08:32:06.740307Z","shell.execute_reply":"2025-01-18T08:32:06.757608Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n# Dataset class definition\nclass TwitterDataset(Dataset):\n    def __init__(self, texts, labels, vocab, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.vocab = vocab\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        \n        # Convert text to tokens\n        tokens = [self.vocab.get(word, self.vocab['<UNK>']) for word in str(text).split()]\n        \n        # Pad or truncate\n        if len(tokens) < self.max_length:\n            tokens = tokens + [self.vocab['<PAD>']] * (self.max_length - len(tokens))\n        else:\n            tokens = tokens[:self.max_length]\n        \n        return {\n            'input_ids': torch.tensor(tokens, dtype=torch.long),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\ndef build_vocabulary(texts, max_vocab_size=10000):\n    \"\"\"Build vocabulary from texts\"\"\"\n    word_freq = {}\n    for text in texts:\n        for word in str(text).split():\n            word_freq[word] = word_freq.get(word, 0) + 1\n    \n    # Sort words by frequency\n    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n    \n    # Create vocabulary with special tokens\n    vocab = {\n        '<PAD>': 0,\n        '<UNK>': 1,\n    }\n    \n    # Add most frequent words\n    for word, _ in sorted_words[:max_vocab_size-2]:  # -2 for special tokens\n        vocab[word] = len(vocab)\n    \n    return vocab\n\n# Now you can use this with your training code:\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load and preprocess data\ntrain_texts, val_texts, train_labels, val_labels = load_twitter_data(\"/kaggle/input/sentiment-analysis-dataset/train.csv\")\n\n# Build vocabulary\nprint(\"Building vocabulary...\")\nvocab = build_vocabulary(train_texts)\nprint(f\"Vocabulary size: {len(vocab)}\")\n\n# Create data loaders\ntrain_loader = DataLoader(TwitterDataset(train_texts, train_labels, vocab), batch_size=32, shuffle=True)\nval_loader = DataLoader(TwitterDataset(val_texts, val_labels, vocab), batch_size=32)\n\n# Initialize model\nmodel = TwitterSentimentTransformer(\n    vocab_size=len(vocab),\n    d_model=256,\n    num_heads=8,\n    num_layers=4\n).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:32:06.759101Z","iopub.execute_input":"2025-01-18T08:32:06.759341Z","iopub.status.idle":"2025-01-18T08:32:12.217593Z","shell.execute_reply.started":"2025-01-18T08:32:06.759323Z","shell.execute_reply":"2025-01-18T08:32:12.216862Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading and preprocessing data...\nTraining samples: 21933\nValidation samples: 5484\nBuilding vocabulary...\nVocabulary size: 10000\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# [9] Main execution\ntorch.manual_seed(42)\nnp.random.seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\ntrain_texts, val_texts, train_labels, val_labels = load_twitter_data(\"/kaggle/input/sentiment-analysis-dataset/train.csv\")\ntrain_loader = DataLoader(TwitterDataset(train_texts, train_labels, vocab), batch_size=32, shuffle=True)\nval_loader = DataLoader(TwitterDataset(val_texts, val_labels, vocab), batch_size=32)\nmodel = TwitterSentimentTransformer(\n    vocab_size=len(vocab),\n    d_model=256,\n    num_heads=8,\n    num_layers=4\n).to(device)\nhistory = train_model(model, train_loader, val_loader, epochs=10, device=device)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:32:12.218534Z","iopub.execute_input":"2025-01-18T08:32:12.218781Z","iopub.status.idle":"2025-01-18T08:37:15.484360Z","shell.execute_reply.started":"2025-01-18T08:32:12.218762Z","shell.execute_reply":"2025-01-18T08:37:15.483667Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading and preprocessing data...\nTraining samples: 21933\nValidation samples: 5484\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 686/686 [00:25<00:00, 26.69it/s]\nEpoch 2/10: 100%|██████████| 686/686 [00:25<00:00, 26.97it/s]\nEpoch 3/10: 100%|██████████| 686/686 [00:26<00:00, 25.96it/s]\nEpoch 4/10: 100%|██████████| 686/686 [00:27<00:00, 24.77it/s]\nEpoch 5/10: 100%|██████████| 686/686 [00:28<00:00, 23.70it/s]\nEpoch 6/10: 100%|██████████| 686/686 [00:27<00:00, 24.68it/s]\nEpoch 7/10: 100%|██████████| 686/686 [00:27<00:00, 24.66it/s]\nEpoch 8/10: 100%|██████████| 686/686 [00:28<00:00, 24.37it/s]\nEpoch 9/10: 100%|██████████| 686/686 [00:28<00:00, 24.48it/s]\nEpoch 10/10: 100%|██████████| 686/686 [00:27<00:00, 24.65it/s]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\n\n# Assuming `model` is your trained PyTorch model\ntorch.save(model.state_dict(), 'model.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T08:41:27.759869Z","iopub.execute_input":"2025-01-18T08:41:27.760233Z","iopub.status.idle":"2025-01-18T08:41:27.818570Z","shell.execute_reply.started":"2025-01-18T08:41:27.760196Z","shell.execute_reply":"2025-01-18T08:41:27.817705Z"}},"outputs":[],"execution_count":16}]}